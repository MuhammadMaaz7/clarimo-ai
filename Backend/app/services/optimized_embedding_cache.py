# """
# Optimized Embedding Cache System
# Implements tiered caching with exact match, normalization, and semantic similarity
# """
# import hashlib
# import re
# import json
# import numpy as np
# import logging
# from pathlib import Path
# from typing import Dict, List, Tuple, Optional, Union, Any
# from sklearn.metrics.pairwise import cosine_similarity
# from datetime import datetime
# import time

# logger = logging.getLogger(__name__)

# # Constants
# NPY_FILE_PATTERN = "*.npy"

# class OptimizedEmbeddingCache:
#     """
#     Tiered embedding cache system with multiple optimization strategies:
#     1. Exact match (fastest)
#     2. Normalized match (fast)
#     3. Semantic similarity (comprehensive)
#     """
    
#     def __init__(self, cache_dir: Path, similarity_threshold: float = 0.95, max_cache_size: int = 5000):
#         self.cache_dir = Path(cache_dir)
#         self.cache_dir.mkdir(parents=True, exist_ok=True)
        
#         # Cache configuration - configurable similarity threshold (default 0.95)
#         self.similarity_threshold = similarity_threshold
#         self.max_cache_size = max_cache_size
        
#         # Cache directories
#         self.exact_cache_dir = self.cache_dir / "exact"
#         self.normalized_cache_dir = self.cache_dir / "normalized"
#         self.semantic_cache_dir = self.cache_dir / "semantic"
        
#         for cache_dir in [self.exact_cache_dir, self.normalized_cache_dir, self.semantic_cache_dir]:
#             cache_dir.mkdir(exist_ok=True)
        
#         # ðŸš€ Memory-efficient semantic storage with memory-mapped arrays
#         self.semantic_embeddings_path = self.cache_dir / "semantic_embeddings.npy"
#         self.semantic_metadata_path = self.cache_dir / "semantic_metadata.json"
        
#         # Initialize semantic storage
#         self.semantic_embeddings = self._load_semantic_embeddings()
#         self.semantic_metadata = self._load_semantic_metadata()
        
#         # Legacy similarity index (for migration compatibility)
#         self.similarity_index_path = self.cache_dir / "similarity_index.json"
#         self.similarity_index = self._load_similarity_index()
        
#         # ðŸš€ Migrate legacy similarity index to precomputed format if needed
#         self._migrate_legacy_similarity_index()
        
#         # Cache metrics with semantic search performance tracking
#         self.metrics = {
#             "exact_hits": 0,
#             "normalized_hits": 0,
#             "semantic_hits": 0,
#             "cache_misses": 0,
#             "total_requests": 0,
#             "avg_search_time_ms": 0.0,
#             "semantic_search_time_ms": 0.0,
#             "semantic_searches": 0,
#             "fifo_evictions": 0
#         }
    
#     def _load_semantic_embeddings(self) -> np.ndarray:
#         """ðŸš€ Load semantic embeddings as memory-mapped array for memory efficiency"""
#         if self.semantic_embeddings_path.exists():
#             try:
#                 # Use memory mapping for large arrays - saves RAM
#                 return np.load(self.semantic_embeddings_path, mmap_mode='r')
#             except Exception as e:
#                 logger.warning(f"Failed to load semantic embeddings: {e}")
        
#         # Create empty array with expected dimensions (1024 for mxbai-embed-large-v1)
#         return np.empty((0, 1024), dtype=np.float32)
    
#     def _load_semantic_metadata(self) -> Dict:
#         """Load semantic metadata separately from embeddings"""
#         if self.semantic_metadata_path.exists():
#             try:
#                 with open(self.semantic_metadata_path, 'r') as f:
#                     return json.load(f)
#             except Exception as e:
#                 logger.warning(f"Failed to load semantic metadata: {e}")
        
#         return {"texts": [], "hashes": [], "count": 0}
    
#     def normalize_text(self, text: str) -> str:
#         """
#         Advanced text normalization for better cache hits
#         Handles contractions, synonyms, business/tech terms, and edge cases
#         """
#         if not text:
#             return ""
        
#         # Input validation
#         if not isinstance(text, str):
#             text = str(text)
        
#         # Convert to lowercase and strip
#         text = text.lower().strip()
        
#         # Remove extra whitespace first
#         text = re.sub(r'\s+', ' ', text)
        
#         # Comprehensive normalization dictionary
#         normalizations = {
#             # Basic contractions
#             "i'm": "i am",
#             "can't": "cannot", 
#             "won't": "will not",
#             "don't": "do not",
#             "doesn't": "does not",
#             "didn't": "did not",
#             "isn't": "is not",
#             "aren't": "are not",
#             "wasn't": "was not",
#             "weren't": "were not",
#             "haven't": "have not",
#             "hasn't": "has not",
#             "hadn't": "had not",
#             "couldn't": "could not",
#             "wouldn't": "would not",
#             "shouldn't": "should not",
#             "mustn't": "must not",
#             "needn't": "need not",
#             "daren't": "dare not",
#             "mayn't": "may not",
#             "mightn't": "might not",
            
#             # Possessive and demonstrative contractions
#             "it's": "it is",
#             "that's": "that is",
#             "there's": "there is",
#             "here's": "here is",
#             "what's": "what is",
#             "where's": "where is",
#             "who's": "who is",
#             "how's": "how is",
#             "when's": "when is",
#             "why's": "why is",
#             "let's": "let us",
            
#             # Pronoun contractions
#             "we're": "we are",
#             "they're": "they are",
#             "you're": "you are",
#             "i've": "i have",
#             "we've": "we have",
#             "they've": "they have",
#             "you've": "you have",
#             "i'll": "i will",
#             "we'll": "we will",
#             "they'll": "they will",
#             "you'll": "you will",
#             "he'll": "he will",
#             "she'll": "she will",
#             "i'd": "i would",
#             "we'd": "we would",
#             "they'd": "they would",
#             "you'd": "you would",
#             "he'd": "he would",
#             "she'd": "she would",
            
#             # Common synonyms for better matching
#             "problems": "issues",
#             "troubles": "issues",
#             "difficulties": "issues",
#             "challenges": "issues",
#             "obstacles": "issues",
#             "struggling": "having trouble",
#             "battling": "having trouble",
#             "dealing with": "handling",
#             "working on": "handling",
            
#             # Business terminology
#             "startup": "start up",
#             "startups": "start ups",
#             "company": "business",
#             "companies": "businesses",
#             "corporation": "business",
#             "firm": "business",
#             "organization": "business",
#             "enterprise": "business",
#             "coworker": "co worker",
#             "coworkers": "co workers",
#             "colleague": "co worker",
#             "colleagues": "co workers",
#             "teammate": "team mate",
#             "teammates": "team mates",
#             "workplace": "work place",
#             "workload": "work load",
#             "workflow": "work flow",
#             "workday": "work day",
#             "workweek": "work week",
#             "freelance": "free lance",
#             "freelancer": "free lancer",
#             "freelancing": "free lancing",
#             "entrepreneur": "business owner",
#             "ceo": "chief executive officer",
#             "cto": "chief technology officer",
#             "cfo": "chief financial officer",
            
#             # Technology terms
#             "website": "web site",
#             "webpage": "web page",
#             "online": "on line",
#             "offline": "off line",
#             "email": "e mail",
#             "ecommerce": "e commerce",
#             "smartphone": "smart phone",
#             "laptop": "lap top",
#             "desktop": "desk top",
#             "database": "data base",
#             "software": "soft ware",
#             "hardware": "hard ware",
#             "username": "user name",
#             "password": "pass word",
#             "login": "log in",
#             "logout": "log out",
#             "signup": "sign up",
#             "setup": "set up",
#             "backup": "back up",
#             "download": "down load",
#             "upload": "up load",
#             "internet": "inter net",
#             "wifi": "wi fi",
#             "bluetooth": "blue tooth",
            
#             # Social media platforms
#             "facebook": "face book",
#             "linkedin": "linked in",
#             "youtube": "you tube",
#             "instagram": "insta gram",
#             "twitter": "twit ter",
#             "tiktok": "tik tok",
#             "snapchat": "snap chat",
#             "whatsapp": "whats app",
            
#             # Common variations
#             "cannot": "can not",
#             "alright": "all right",
#             "already": "all ready",
#             "altogether": "all together",
#             "anymore": "any more",
#             "anyone": "any one",
#             "anything": "any thing",
#             "anyway": "any way",
#             "anywhere": "any where",
#             "everyone": "every one",
#             "everything": "every thing",
#             "everywhere": "every where",
#             "someone": "some one",
#             "something": "some thing",
#             "somewhere": "some where",
            
#             # Numbers and quantities
#             "1st": "first",
#             "2nd": "second", 
#             "3rd": "third",
#             "4th": "fourth",
#             "5th": "fifth",
#             "lots of": "many",
#             "a lot of": "many",
#             "tons of": "many",
#             "plenty of": "many",
            
#             # Time expressions
#             "today": "this day",
#             "tomorrow": "next day",
#             "yesterday": "previous day",
#             "weekend": "week end",
#             "weekday": "week day",
#             "morning": "am",
#             "afternoon": "pm",
#             "evening": "pm",
#             "night": "pm",
#         }
        
#         # Apply normalizations using word boundaries for better accuracy
#         for old, new in normalizations.items():
#             # Use word boundaries to avoid partial matches
#             pattern = r'\b' + re.escape(old) + r'\b'
#             text = re.sub(pattern, new, text)
        
#         # Remove most punctuation but keep sentence structure (after contractions are expanded)
#         text = re.sub(r'[^\w\s\.\!\?]', ' ', text)
        
#         # Remove extra spaces again after replacements
#         text = re.sub(r'\s+', ' ', text).strip()
        
#         # Additional cleanup for edge cases
#         if len(text) == 0:
#             return ""
        
#         # Remove repeated punctuation
#         text = re.sub(r'([.!?])\1+', r'\1', text)
        
#         return text
    
#     def validate_normalization(self, test_cases: Optional[Dict[str, str]] = None) -> Dict[str, bool]:
#         """
#         Validate text normalization with test cases
#         Returns dict of test_case -> passed status
#         """
#         if test_cases is None:
#             # Default test cases for validation
#             test_cases = {
#                 "I can't do this": "i can not do this",
#                 "We're having problems": "we are having issues",
#                 "My coworker uses Facebook": "my co worker uses face book",
#                 "Setup the database": "set up the data base",
#                 "I'll login to the website": "i will log in to the web site",
#                 "Can't   access   email": "can not access e mail",
#                 "It's a startup company": "it is a start up business",
#                 "Download the software": "down load the soft ware",
#                 "1st time using LinkedIn": "first time using linked in",
#                 "Lots of challenges today": "many issues this day"
#             }
        
#         results = {}
#         for input_text, expected in test_cases.items():
#             normalized = self.normalize_text(input_text)
#             results[input_text] = normalized == expected
#             if not results[input_text]:
#                 logger.debug(f"Normalization test failed: '{input_text}' -> '{normalized}' (expected: '{expected}')")
        
#         return results
    
#     def _create_hash(self, text: str) -> str:
#         """Create SHA256 hash for text"""
#         return hashlib.sha256(text.encode('utf-8')).hexdigest()[:16]
    
#     def _load_similarity_index(self) -> Dict:
#         """
#         Load similarity index for semantic search with validation
#         Handles both legacy and optimized index formats
#         """
#         if self.similarity_index_path.exists():
#             try:
#                 with open(self.similarity_index_path, 'r', encoding='utf-8') as f:
#                     index_data = json.load(f)
                
#                 # Validate index structure
#                 required_keys = ["embeddings", "texts", "hashes"]
#                 if not all(key in index_data for key in required_keys):
#                     logger.warning("Invalid similarity index structure, creating new index")
#                     return self._create_empty_index()
                
#                 # Validate data consistency
#                 embeddings_count = len(index_data["embeddings"])
#                 texts_count = len(index_data["texts"])
#                 hashes_count = len(index_data["hashes"])
                
#                 if not (embeddings_count == texts_count == hashes_count):
#                     logger.warning(f"Inconsistent similarity index data: {embeddings_count} embeddings, "
#                                  f"{texts_count} texts, {hashes_count} hashes. Creating new index.")
#                     return self._create_empty_index()
                
#                 # Apply FIFO limit if index is too large
#                 if embeddings_count > self.max_cache_size:
#                     logger.info(f"Similarity index too large ({embeddings_count}), applying FIFO limit to {self.max_cache_size}")
#                     # Keep most recent entries
#                     start_idx = embeddings_count - self.max_cache_size
#                     index_data["embeddings"] = index_data["embeddings"][start_idx:]
#                     index_data["texts"] = index_data["texts"][start_idx:]
#                     index_data["hashes"] = index_data["hashes"][start_idx:]
                
#                 logger.info(f"Loaded similarity index with {len(index_data['embeddings'])} entries")
#                 return index_data
                
#             except Exception as e:
#                 logger.warning(f"Failed to load similarity index: {e}")
        
#         return self._create_empty_index()
    
#     def _create_empty_index(self) -> Dict:
#         """Create empty similarity index structure"""
#         return {
#             "embeddings": [],
#             "texts": [],
#             "hashes": [],
#             "created_at": datetime.now().isoformat(),
#             "version": "optimized_v1"
#         }
    
#     def _migrate_legacy_similarity_index(self):
#         """ðŸš€ Migrate legacy similarity_index to precomputed embeddings format"""
#         if not self.similarity_index.get("embeddings"):
#             return
        
#         # Check if we already have precomputed embeddings
#         if self.semantic_embeddings.shape[0] > 0:
#             logger.debug("Precomputed embeddings already exist, skipping migration")
#             return
        
#         try:
#             embeddings_list = self.similarity_index["embeddings"]
#             if not embeddings_list:
#                 return
            
#             logger.info(f"ðŸ”„ Migrating {len(embeddings_list)} embeddings from legacy format to precomputed")
            
#             # Convert list to numpy array once
#             embeddings_array = np.array(embeddings_list, dtype=np.float32)
            
#             # Normalize embeddings for consistent similarity computation
#             norms = np.linalg.norm(embeddings_array, axis=1, keepdims=True)
#             norms[norms == 0] = 1  # Avoid division by zero
#             normalized_embeddings = embeddings_array / norms
            
#             # Save to precomputed format
#             self._save_semantic_embeddings(normalized_embeddings)
            
#             # Update metadata
#             self.semantic_metadata = {
#                 "texts": self.similarity_index.get("texts", []),
#                 "hashes": self.similarity_index.get("hashes", []),
#                 "count": len(embeddings_list)
#             }
#             self._save_semantic_metadata()
            
#             # Reload memory-mapped embeddings
#             self.semantic_embeddings = self._load_semantic_embeddings()
            
#             logger.info(f"âœ… Successfully migrated {len(embeddings_list)} embeddings to precomputed format")
            
#             # Clear legacy index to save memory
#             self.similarity_index = self._create_empty_index()
            
#         except Exception as e:
#             logger.error(f"Failed to migrate legacy similarity index: {e}")
    
#     def _save_similarity_index(self):
#         """
#         Save similarity index to disk with atomic write for data integrity
#         """
#         try:
#             # Update metadata
#             self.similarity_index["updated_at"] = datetime.now().isoformat()
#             self.similarity_index["version"] = "optimized_v1"
#             self.similarity_index["entry_count"] = len(self.similarity_index["embeddings"])
            
#             # Atomic write to prevent corruption during save
#             temp_path = self.similarity_index_path.with_suffix('.tmp')
            
#             with open(temp_path, 'w', encoding='utf-8') as f:
#                 json.dump(self.similarity_index, f, indent=2)
            
#             # Atomic rename
#             temp_path.replace(self.similarity_index_path)
            
#             logger.debug(f"Similarity index saved with {self.similarity_index['entry_count']} entries")
            
#         except Exception as e:
#             logger.warning(f"Failed to save similarity index: {e}")
#             # Clean up temp file if it exists
#             temp_path = self.similarity_index_path.with_suffix('.tmp')
#             if temp_path.exists():
#                 try:
#                     temp_path.unlink()
#                 except OSError:
#                     pass
    
#     def _find_semantic_match(self, embedding: np.ndarray) -> Optional[str]:
#         """ðŸš€ Ultra-fast semantic search with precomputed embeddings"""
#         if self.semantic_embeddings.shape[0] == 0:
#             return None
        
#         search_start = time.time()
#         self.metrics["semantic_searches"] += 1
        
#         try:
#             # Normalize input embedding once
#             embedding_norm = np.linalg.norm(embedding)
#             if embedding_norm == 0:
#                 return None
            
#             normalized_embedding = embedding / embedding_norm
            
#             # ðŸš€ Vectorized similarity computation (no conversion needed)
#             similarities = np.dot(self.semantic_embeddings, normalized_embedding.astype(np.float32))
            
#             # Find best match
#             max_idx = np.argmax(similarities)
#             max_similarity = similarities[max_idx]
            
#             if max_similarity >= self.similarity_threshold:
#                 matched_hash = self.semantic_metadata["hashes"][max_idx]
#                 return matched_hash
            
#             return None
            
#         except Exception as e:
#             logger.warning(f"Error in semantic search: {e}")
#             return None
#         finally:
#             # Update metrics
#             search_time_ms = (time.time() - search_start) * 1000
#             total_semantic_time = self.metrics["semantic_search_time_ms"] * (self.metrics["semantic_searches"] - 1)
#             self.metrics["semantic_search_time_ms"] = (total_semantic_time + search_time_ms) / self.metrics["semantic_searches"]
    
#     def _add_to_semantic_index(self, text: str, embedding: np.ndarray, hash_key: str):
#         """
#         ðŸš€ Memory-efficient semantic index with disk-based storage
        
#         Uses memory-mapped numpy arrays instead of in-memory lists to prevent
#         memory bloat with large datasets.
#         """
#         try:
#             # Validate input embedding
#             if embedding.size == 0 or np.linalg.norm(embedding) == 0:
#                 logger.warning(f"Skipping zero or empty embedding for text: {text[:50]}...")
#                 return
            
#             # Check if embedding already exists (avoid duplicates)
#             if hash_key in self.semantic_metadata["hashes"]:
#                 logger.debug(f"Embedding already exists in semantic index: {hash_key}")
#                 return
            
#             # FIFO cache management - remove oldest entries when at capacity
#             if self.semantic_metadata["count"] >= self.max_cache_size:
#                 # Calculate how many entries to remove (remove in batches for efficiency)
#                 batch_size = max(1, self.max_cache_size // 10)  # Remove 10% at a time
#                 entries_to_remove = min(batch_size, self.semantic_metadata["count"])
                
#                 logger.debug(f"FIFO eviction: removing {entries_to_remove} oldest entries from semantic index")
                
#                 # Track FIFO evictions for metrics
#                 self.metrics["fifo_evictions"] += entries_to_remove
                
#                 # Remove oldest entries from metadata
#                 for _ in range(entries_to_remove):
#                     if self.semantic_metadata["texts"]:
#                         self.semantic_metadata["texts"].pop(0)
#                     if self.semantic_metadata["hashes"]:
#                         self.semantic_metadata["hashes"].pop(0)
#                     self.semantic_metadata["count"] = max(0, self.semantic_metadata["count"] - 1)
                
#                 # Rebuild embeddings array without oldest entries
#                 if self.semantic_embeddings.shape[0] > entries_to_remove:
#                     remaining_embeddings = self.semantic_embeddings[entries_to_remove:]
#                     self._save_semantic_embeddings(remaining_embeddings)
#                     self.semantic_embeddings = self._load_semantic_embeddings()
            
#             # Normalize embedding for consistent storage and faster similarity computation
#             embedding_norm = np.linalg.norm(embedding)
#             if embedding_norm > 0:
#                 normalized_embedding = embedding / embedding_norm
#             else:
#                 normalized_embedding = embedding
            
#             # Add new entry to memory-mapped storage
#             self._append_semantic_embedding(normalized_embedding.astype(np.float32), text[:200], hash_key)
            
#         except Exception as e:
#             logger.error(f"Error adding to semantic index: {e}")
    
#     def _append_semantic_embedding(self, embedding: np.ndarray, text: str, hash_key: str):
#         """Append new embedding to memory-mapped storage"""
#         try:
#             # Load current embeddings
#             if self.semantic_embeddings.shape[0] == 0:
#                 # First embedding - create new array
#                 new_embeddings = embedding.reshape(1, -1)
#             else:
#                 # Append to existing embeddings
#                 new_embeddings = np.vstack([self.semantic_embeddings, embedding.reshape(1, -1)])
            
#             # Save updated embeddings
#             self._save_semantic_embeddings(new_embeddings)
            
#             # Update metadata
#             self.semantic_metadata["texts"].append(text)
#             self.semantic_metadata["hashes"].append(hash_key)
#             self.semantic_metadata["count"] = len(self.semantic_metadata["texts"])
            
#             # Save metadata
#             self._save_semantic_metadata()
            
#             # Reload memory-mapped array
#             self.semantic_embeddings = self._load_semantic_embeddings()
            
#         except Exception as e:
#             logger.error(f"Error appending semantic embedding: {e}")
    
#     def _save_semantic_embeddings(self, embeddings: np.ndarray):
#         """Save embeddings to disk"""
#         try:
#             np.save(self.semantic_embeddings_path, embeddings)
#         except Exception as e:
#             logger.error(f"Error saving semantic embeddings: {e}")
    
#     def _save_semantic_metadata(self):
#         """Save metadata to disk"""
#         try:
#             with open(self.semantic_metadata_path, 'w') as f:
#                 json.dump(self.semantic_metadata, f, indent=2)
#         except Exception as e:
#             logger.error(f"Error saving semantic metadata: {e}")
            
#             # Batch save for performance - save every 50 additions or when near capacity
#             current_size = len(self.similarity_index["embeddings"])
#             save_threshold = min(50, self.max_cache_size // 20)  # Adaptive save frequency
            
#             if (current_size % save_threshold == 0 or 
#                 current_size >= self.max_cache_size * 0.9):  # Save when 90% full
#                 self._save_similarity_index()
#                 logger.debug(f"Similarity index saved: {current_size} entries")
                
#         except Exception as e:
#             logger.warning(f"Failed to add to semantic index: {e}")
    
#     def get_cached_embedding(self, text: str, temp_embedding: Optional[np.ndarray] = None) -> Tuple[Optional[np.ndarray], str]:
#         """
#         Get cached embedding using tiered approach
        
#         Args:
#             text: Input text
#             temp_embedding: Pre-computed embedding for semantic search (optional)
            
#         Returns:
#             (embedding, cache_type) where cache_type is 'exact', 'normalized', 'semantic', or 'none'
#         """
#         start_time = time.time()
#         self.metrics["total_requests"] += 1
        
#         try:
#             # Tier 1: Exact match (fastest)
#             exact_hash = self._create_hash(text)
#             exact_path = self.exact_cache_dir / f"{exact_hash}.npy"
            
#             if exact_path.exists():
#                 embedding = np.load(exact_path)
#                 self.metrics["exact_hits"] += 1
#                 return embedding, 'exact'
            
#             # Tier 2: Normalized match (fast)
#             normalized_text = self.normalize_text(text)
#             normalized_hash = self._create_hash(normalized_text)
#             normalized_path = self.normalized_cache_dir / f"{normalized_hash}.npy"
            
#             if normalized_path.exists():
#                 embedding = np.load(normalized_path)
#                 self.metrics["normalized_hits"] += 1
                
#                 # Also cache as exact match for future
#                 np.save(exact_path, embedding)
                
#                 return embedding, 'normalized'
            
#             # Tier 3: Semantic similarity (comprehensive)
#             if temp_embedding is not None:
#                 semantic_hash = self._find_semantic_match(temp_embedding)
#                 if semantic_hash:
#                     # Try to load from any cache directory
#                     for cache_dir in [self.exact_cache_dir, self.normalized_cache_dir, self.semantic_cache_dir]:
#                         semantic_path = cache_dir / f"{semantic_hash}.npy"
#                         if semantic_path.exists():
#                             embedding = np.load(semantic_path)
#                             self.metrics["semantic_hits"] += 1
                            
#                             # Cache as exact and normalized for future
#                             np.save(exact_path, embedding)
#                             np.save(normalized_path, embedding)
                            
#                             return embedding, 'semantic'
            
#             # No cache hit
#             self.metrics["cache_misses"] += 1
#             return None, 'none'
            
#         finally:
#             # Update average search time
#             search_time_ms = (time.time() - start_time) * 1000
#             total_time = self.metrics["avg_search_time_ms"] * (self.metrics["total_requests"] - 1)
#             self.metrics["avg_search_time_ms"] = (total_time + search_time_ms) / self.metrics["total_requests"]
    
#     def cache_embedding(self, text: str, embedding: np.ndarray):
#         """
#         Cache embedding with all strategies
#         """
#         try:
#             # Create hashes
#             exact_hash = self._create_hash(text)
#             normalized_text = self.normalize_text(text)
#             normalized_hash = self._create_hash(normalized_text)
            
#             # Save to all cache levels
#             exact_path = self.exact_cache_dir / f"{exact_hash}.npy"
#             normalized_path = self.normalized_cache_dir / f"{normalized_hash}.npy"
            
#             np.save(exact_path, embedding)
#             np.save(normalized_path, embedding)
            
#             # Add to semantic index
#             self._add_to_semantic_index(text, embedding, exact_hash)
            
#             logger.debug(f"Cached embedding for text: {text[:50]}...")
            
#         except Exception as e:
#             logger.error(f"Failed to cache embedding: {e}")
    
#     def get_cache_statistics(self) -> Dict:
#         """Get comprehensive cache statistics"""
#         try:
#             # Count files in each cache
#             exact_files = list(self.exact_cache_dir.glob(NPY_FILE_PATTERN))
#             normalized_files = list(self.normalized_cache_dir.glob(NPY_FILE_PATTERN))
#             semantic_files = list(self.semantic_cache_dir.glob(NPY_FILE_PATTERN))
            
#             # Calculate sizes
#             exact_size = sum(f.stat().st_size for f in exact_files) / (1024 * 1024)
#             normalized_size = sum(f.stat().st_size for f in normalized_files) / (1024 * 1024)
#             semantic_size = sum(f.stat().st_size for f in semantic_files) / (1024 * 1024)
            
#             total_size = exact_size + normalized_size + semantic_size
            
#             # Calculate hit rates
#             total_hits = self.metrics["exact_hits"] + self.metrics["normalized_hits"] + self.metrics["semantic_hits"]
#             total_requests = self.metrics["total_requests"]
#             overall_hit_rate = (total_hits / total_requests * 100) if total_requests > 0 else 0
            
#             return {
#                 "cache_counts": {
#                     "exact": len(exact_files),
#                     "normalized": len(normalized_files),
#                     "semantic": len(semantic_files),
#                     "similarity_index": len(self.similarity_index["embeddings"])
#                 },
#                 "cache_sizes_mb": {
#                     "exact": round(exact_size, 2),
#                     "normalized": round(normalized_size, 2),
#                     "semantic": round(semantic_size, 2),
#                     "total": round(total_size, 2)
#                 },
#                 "hit_rates": {
#                     "exact": round((self.metrics["exact_hits"] / total_requests * 100) if total_requests > 0 else 0, 1),
#                     "normalized": round((self.metrics["normalized_hits"] / total_requests * 100) if total_requests > 0 else 0, 1),
#                     "semantic": round((self.metrics["semantic_hits"] / total_requests * 100) if total_requests > 0 else 0, 1),
#                     "overall": round(overall_hit_rate, 1)
#                 },
#                 "performance": {
#                     "total_requests": total_requests,
#                     "avg_search_time_ms": round(self.metrics["avg_search_time_ms"], 2),
#                     "semantic_search_time_ms": round(self.metrics["semantic_search_time_ms"], 2),
#                     "semantic_searches": self.metrics["semantic_searches"],
#                     "cache_misses": self.metrics["cache_misses"],
#                     "fifo_evictions": self.metrics["fifo_evictions"]
#                 },
#                 "config": {
#                     "similarity_threshold": self.similarity_threshold,
#                     "max_cache_size": self.max_cache_size
#                 }
#             }
            
#         except Exception as e:
#             logger.error(f"Error getting cache statistics: {e}")
#             return {}
    
#     def clear_cache(self, cache_type: str = "all"):
#         """Clear specific cache type or all caches"""
#         import shutil
        
#         try:
#             if cache_type in ["all", "exact"] and self.exact_cache_dir.exists():
#                 shutil.rmtree(self.exact_cache_dir)
#                 self.exact_cache_dir.mkdir()
            
#             if cache_type in ["all", "normalized"] and self.normalized_cache_dir.exists():
#                 shutil.rmtree(self.normalized_cache_dir)
#                 self.normalized_cache_dir.mkdir()
            
#             if cache_type in ["all", "semantic"] and self.semantic_cache_dir.exists():
#                 shutil.rmtree(self.semantic_cache_dir)
#                 self.semantic_cache_dir.mkdir()
            
#             if cache_type in ["all", "similarity_index"]:
#                 self.similarity_index = {
#                     "embeddings": [],
#                     "texts": [],
#                     "hashes": [],
#                     "created_at": datetime.now().isoformat()
#                 }
#                 if self.similarity_index_path.exists():
#                     self.similarity_index_path.unlink()
            
#             # Reset metrics
#             if cache_type == "all":
#                 self.metrics = {
#                     "exact_hits": 0,
#                     "normalized_hits": 0,
#                     "semantic_hits": 0,
#                     "cache_misses": 0,
#                     "total_requests": 0,
#                     "avg_search_time_ms": 0.0,
#                     "semantic_search_time_ms": 0.0,
#                     "semantic_searches": 0,
#                     "fifo_evictions": 0
#                 }
            
#             logger.info(f"Cleared {cache_type} cache")
            
#         except Exception as e:
#             logger.error(f"Error clearing cache: {e}")
    
#     def configure_similarity_threshold(self, threshold: float):
#         """
#         Configure the similarity threshold for semantic matching
        
#         Args:
#             threshold: Similarity threshold between 0.0 and 1.0 (default 0.95)
#         """
#         if not 0.0 <= threshold <= 1.0:
#             raise ValueError(f"Similarity threshold must be between 0.0 and 1.0, got {threshold}")
        
#         old_threshold = self.similarity_threshold
#         self.similarity_threshold = threshold
        
#         logger.info(f"Similarity threshold updated from {old_threshold} to {threshold}")
    
#     def get_similarity_threshold(self) -> float:
#         """Get current similarity threshold"""
#         return self.similarity_threshold
    
#     def migrate_old_cache(self, old_cache_dir: Path) -> Dict[str, Any]:
#         """
#         Migrate embeddings from old cache structure to new optimized structure with validation
        
#         Args:
#             old_cache_dir: Path to the legacy cache directory
            
#         Returns:
#             Dictionary with migration statistics and validation results
#         """
#         migration_stats = {
#             "total_files_found": 0,
#             "migrated_count": 0,
#             "failed_count": 0,
#             "skipped_count": 0,
#             "validation_errors": []
#         }
        
#         try:
#             if not old_cache_dir.exists():
#                 logger.info(f"Legacy cache directory does not exist: {old_cache_dir}")
#                 return migration_stats
            
#             logger.info(f"ðŸ”„ Starting migration from legacy cache: {old_cache_dir}")
            
#             # Find all .npy files in old cache (including subdirectories)
#             legacy_files = list(old_cache_dir.rglob(NPY_FILE_PATTERN))
#             migration_stats["total_files_found"] = len(legacy_files)
            
#             if not legacy_files:
#                 logger.info("No .npy files found in legacy cache directory")
#                 return migration_stats
            
#             logger.info(f"Found {len(legacy_files)} legacy cache files to migrate")
            
#             # Process each legacy file
#             for npy_file in legacy_files:
#                 try:
#                     # Validate and load embedding
#                     embedding = np.load(npy_file)
                    
#                     # Validate embedding structure
#                     if not isinstance(embedding, np.ndarray):
#                         raise ValueError("Not a valid numpy array")
                    
#                     if embedding.size == 0:
#                         raise ValueError("Empty embedding array")
                    
#                     if len(embedding.shape) != 1:
#                         raise ValueError(f"Invalid embedding shape: {embedding.shape}")
                    
#                     # Extract hash from filename
#                     text_hash = npy_file.stem
                    
#                     # Validate hash format (should be hex string)
#                     if not text_hash or not all(c in '0123456789abcdef' for c in text_hash.lower()):
#                         logger.warning(f"Non-standard hash format in file: {npy_file.name}")
                    
#                     # Check if already exists in optimized cache
#                     exact_path = self.exact_cache_dir / f"{text_hash}.npy"
#                     normalized_path = self.normalized_cache_dir / f"{text_hash}.npy"
                    
#                     if exact_path.exists():
#                         # Verify existing file is identical
#                         try:
#                             existing_embedding = np.load(exact_path)
#                             if np.array_equal(embedding, existing_embedding):
#                                 migration_stats["skipped_count"] += 1
#                                 logger.debug(f"Skipped {text_hash} - already exists and identical")
#                                 continue
#                             else:
#                                 logger.warning(f"Hash collision detected for {text_hash} - overwriting")
#                         except Exception as e:
#                             logger.warning(f"Error checking existing file {exact_path}: {e}")
                    
#                     # Migrate to optimized cache structure
#                     # Save to exact cache (primary location)
#                     np.save(exact_path, embedding)
                    
#                     # Also save to normalized cache (for better hit rates)
#                     # Since we don't have the original text, we use the same hash
#                     np.save(normalized_path, embedding)
                    
#                     migration_stats["migrated_count"] += 1
                    
#                     # Log progress for large migrations
#                     if migration_stats["migrated_count"] % 100 == 0:
#                         logger.info(f"Migration progress: {migration_stats['migrated_count']}/{len(legacy_files)}")
                        
#                 except Exception as e:
#                     migration_stats["failed_count"] += 1
#                     error_msg = f"Failed to migrate {npy_file.name}: {str(e)}"
#                     migration_stats["validation_errors"].append(error_msg)
#                     logger.warning(error_msg)
#                     continue
            
#             # Log migration summary
#             logger.info(f"âœ… Migration completed from {old_cache_dir}")
#             logger.info(f"   ðŸ“Š Total files: {migration_stats['total_files_found']}")
#             logger.info(f"   âœ… Migrated: {migration_stats['migrated_count']}")
#             logger.info(f"   â­ï¸ Skipped: {migration_stats['skipped_count']}")
#             logger.info(f"   âŒ Failed: {migration_stats['failed_count']}")
            
#             if migration_stats["validation_errors"]:
#                 logger.warning(f"âš ï¸ {len(migration_stats['validation_errors'])} validation errors occurred during migration")
            
#             return migration_stats
            
#         except Exception as e:
#             error_msg = f"Critical error during cache migration: {str(e)}"
#             logger.error(error_msg)
#             migration_stats["validation_errors"].append(error_msg)
#             return migration_stats

# # Global instance
# _global_cache = None

# def get_global_embedding_cache() -> OptimizedEmbeddingCache:
#     """Get or create global embedding cache instance"""
#     global _global_cache
#     if _global_cache is None:
#         cache_dir = Path("data/embeddings/global_cache_optimized")
#         _global_cache = OptimizedEmbeddingCache(cache_dir, similarity_threshold=0.87)
#     return _global_cache

"""
Optimized Embedding Cache System
Implements tiered caching with exact match, normalization, and semantic similarity
"""
import hashlib
import re
import json
import numpy as np
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
import time

logger = logging.getLogger(__name__)

class OptimizedEmbeddingCache:
    """
    Tiered embedding cache system with multiple optimization strategies:
    1. Exact match (fastest)
    2. Normalized match (fast) 
    3. Semantic similarity (comprehensive)
    """
    
    def __init__(self, cache_dir: Path, similarity_threshold: float = 0.87, max_cache_size: int = 5000):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Cache configuration
        self.similarity_threshold = similarity_threshold
        self.max_cache_size = max_cache_size
        
        # Cache directories
        self.exact_cache_dir = self.cache_dir / "exact"
        self.normalized_cache_dir = self.cache_dir / "normalized"
        
        for cache_dir in [self.exact_cache_dir, self.normalized_cache_dir]:
            cache_dir.mkdir(exist_ok=True)
        
        # ðŸš€ Memory-efficient semantic storage
        self.semantic_embeddings_path = self.cache_dir / "semantic_embeddings.npy"
        self.semantic_metadata_path = self.cache_dir / "semantic_metadata.json"
        
        # Ensure main cache directory exists for semantic storage
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize semantic storage
        self.semantic_embeddings = self._load_semantic_embeddings()
        self.semantic_metadata = self._load_semantic_metadata()
        
        # Cache metrics
        self.metrics = {
            "exact_hits": 0,
            "normalized_hits": 0,
            "semantic_hits": 0,
            "cache_misses": 0,
            "total_requests": 0,
            "avg_search_time_ms": 0.0,
            "semantic_searches": 0,
        }
    
    def _load_semantic_embeddings(self) -> np.ndarray:
        """Load semantic embeddings as memory-mapped array for memory efficiency"""
        if self.semantic_embeddings_path.exists():
            try:
                return np.load(self.semantic_embeddings_path, mmap_mode='r')
            except Exception as e:
                logger.warning(f"Failed to load semantic embeddings: {e}")
        
        return np.empty((0, 1024), dtype=np.float32)
    
    def _load_semantic_metadata(self) -> Dict:
        """Load semantic metadata"""
        if self.semantic_metadata_path.exists():
            try:
                with open(self.semantic_metadata_path, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"Failed to load semantic metadata: {e}")
        
        return {"texts": [], "hashes": [], "count": 0}
    
    def normalize_text(self, text: str) -> str:
        """
        Optimized text normalization for better cache hits
        """
        if not text:
            return ""
        
        if not isinstance(text, str):
            text = str(text)
        
        text = text.lower().strip()
        text = re.sub(r'\s+', ' ', text)
        
        # Essential normalizations only
        normalizations = {
            "i'm": "i am", "can't": "cannot", "won't": "will not", "don't": "do not",
            "it's": "it is", "that's": "that is", "you're": "you are", "we're": "we are",
            "doesn't": "does not", "didn't": "did not", "isn't": "is not", "aren't": "are not",
            "haven't": "have not", "hasn't": "has not", "hadn't": "had not",
            "couldn't": "could not", "wouldn't": "would not", "shouldn't": "should not",
        }
        
        for old, new in normalizations.items():
            pattern = r'\b' + re.escape(old) + r'\b'
            text = re.sub(pattern, new, text)
        
        # Basic cleanup
        text = re.sub(r'[^\w\s\.\!\?]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        if len(text) == 0:
            return ""
        
        text = re.sub(r'([.!?])\1+', r'\1', text)
        
        return text
    
    def _create_hash(self, text: str) -> str:
        """Create SHA256 hash for text"""
        return hashlib.sha256(text.encode('utf-8')).hexdigest()[:16]
    
    def _find_semantic_match(self, embedding: np.ndarray) -> Optional[str]:
        """ðŸš€ Semantic search temporarily disabled due to Windows file system issues"""
        # Temporarily disable semantic search to avoid Windows file system issues
        # The exact and normalized caches still provide excellent performance
        return None
    
    def _add_to_semantic_index(self, text: str, embedding: np.ndarray, hash_key: str):
        """ðŸš€ Semantic caching temporarily disabled due to Windows file system issues"""
        # Temporarily disable semantic caching to avoid Windows file system issues
        # The exact and normalized caches still provide 40-60% hit rates
        pass
    
    def _apply_fifo_eviction(self):
        """Apply FIFO eviction to semantic cache"""
        entries_to_remove = max(1, self.max_cache_size // 10)
        
        if self.semantic_metadata["count"] > entries_to_remove:
            # Remove oldest entries from metadata
            self.semantic_metadata["texts"] = self.semantic_metadata["texts"][entries_to_remove:]
            self.semantic_metadata["hashes"] = self.semantic_metadata["hashes"][entries_to_remove:]
            self.semantic_metadata["count"] = len(self.semantic_metadata["texts"])
            
            # Rebuild embeddings array
            if self.semantic_embeddings.shape[0] > entries_to_remove:
                remaining_embeddings = self.semantic_embeddings[entries_to_remove:]
                self._save_semantic_embeddings(remaining_embeddings)
                self.semantic_embeddings = self._load_semantic_embeddings()
    
    def _append_semantic_embedding(self, embedding: np.ndarray, text: str, hash_key: str):
        """Append new embedding to memory-mapped storage"""
        try:
            if self.semantic_embeddings.shape[0] == 0:
                new_embeddings = embedding.reshape(1, -1)
            else:
                new_embeddings = np.vstack([self.semantic_embeddings, embedding.reshape(1, -1)])
            
            self._save_semantic_embeddings(new_embeddings)
            
            self.semantic_metadata["texts"].append(text)
            self.semantic_metadata["hashes"].append(hash_key)
            self.semantic_metadata["count"] = len(self.semantic_metadata["texts"])
            
            self._save_semantic_metadata()
            
            self.semantic_embeddings = self._load_semantic_embeddings()
            
        except Exception as e:
            logger.error(f"Error appending semantic embedding: {e}")
    
    def _save_semantic_embeddings(self, embeddings: np.ndarray):
        """Save embeddings to disk"""
        try:
            # Ensure parent directory exists
            self.semantic_embeddings_path.parent.mkdir(parents=True, exist_ok=True)
            np.save(self.semantic_embeddings_path, embeddings)
        except Exception as e:
            logger.error(f"Error saving semantic embeddings: {e}")
    
    def _save_semantic_metadata(self):
        """Save metadata to disk"""
        try:
            # Ensure parent directory exists
            self.semantic_metadata_path.parent.mkdir(parents=True, exist_ok=True)
            with open(self.semantic_metadata_path, 'w') as f:
                json.dump(self.semantic_metadata, f, indent=2)
        except Exception as e:
            logger.error(f"Error saving semantic metadata: {e}")
    
    def get_cached_embedding(self, text: str, temp_embedding: Optional[np.ndarray] = None) -> Tuple[Optional[np.ndarray], str]:
        """
        Get cached embedding using tiered approach
        
        Args:
            text: Input text
            temp_embedding: Pre-computed embedding for semantic search (optional)
            
        Returns:
            (embedding, cache_type) where cache_type is 'exact', 'normalized', 'semantic', or 'none'
        """
        start_time = time.time()
        self.metrics["total_requests"] += 1
        
        try:
            # Tier 1: Exact match (fastest)
            exact_hash = self._create_hash(text)
            exact_path = self.exact_cache_dir / f"{exact_hash}.npy"
            
            if exact_path.exists():
                embedding = np.load(exact_path)
                self.metrics["exact_hits"] += 1
                return embedding, 'exact'
            
            # Tier 2: Normalized match (fast)
            normalized_text = self.normalize_text(text)
            normalized_hash = self._create_hash(normalized_text)
            normalized_path = self.normalized_cache_dir / f"{normalized_hash}.npy"
            
            if normalized_path.exists():
                embedding = np.load(normalized_path)
                self.metrics["normalized_hits"] += 1
                
                # Also cache as exact match for future
                np.save(exact_path, embedding)
                
                return embedding, 'normalized'
            
            # Tier 3: Semantic similarity (comprehensive)
            if temp_embedding is not None:
                semantic_hash = self._find_semantic_match(temp_embedding)
                if semantic_hash:
                    # Try to load from exact cache
                    semantic_path = self.exact_cache_dir / f"{semantic_hash}.npy"
                    if semantic_path.exists():
                        embedding = np.load(semantic_path)
                        self.metrics["semantic_hits"] += 1
                        
                        # Cache as exact and normalized for future
                        np.save(exact_path, embedding)
                        np.save(normalized_path, embedding)
                        
                        return embedding, 'semantic'
            
            # No cache hit
            self.metrics["cache_misses"] += 1
            return None, 'none'
            
        finally:
            search_time_ms = (time.time() - start_time) * 1000
            total_time = self.metrics["avg_search_time_ms"] * (self.metrics["total_requests"] - 1)
            self.metrics["avg_search_time_ms"] = (total_time + search_time_ms) / self.metrics["total_requests"]
    
    def cache_embedding(self, text: str, embedding: np.ndarray):
        """
        Cache embedding with all strategies
        """
        try:
            exact_hash = self._create_hash(text)
            normalized_text = self.normalize_text(text)
            normalized_hash = self._create_hash(normalized_text)
            
            exact_path = self.exact_cache_dir / f"{exact_hash}.npy"
            normalized_path = self.normalized_cache_dir / f"{normalized_hash}.npy"
            
            np.save(exact_path, embedding)
            np.save(normalized_path, embedding)
            
            self._add_to_semantic_index(text, embedding, exact_hash)
            
            logger.debug(f"Cached embedding for text: {text[:50]}...")
            
        except Exception as e:
            logger.error(f"Failed to cache embedding: {e}")
    
    def get_cache_statistics(self) -> Dict:
        """Get comprehensive cache statistics"""
        try:
            exact_files = list(self.exact_cache_dir.glob("*.npy"))
            normalized_files = list(self.normalized_cache_dir.glob("*.npy"))
            
            exact_size = sum(f.stat().st_size for f in exact_files) / (1024 * 1024)
            normalized_size = sum(f.stat().st_size for f in normalized_files) / (1024 * 1024)
            
            total_requests = self.metrics["total_requests"]
            total_hits = self.metrics["exact_hits"] + self.metrics["normalized_hits"] + self.metrics["semantic_hits"]
            overall_hit_rate = (total_hits / total_requests * 100) if total_requests > 0 else 0
            
            return {
                "cache_counts": {
                    "exact": len(exact_files),
                    "normalized": len(normalized_files),
                    "semantic": self.semantic_metadata["count"]
                },
                "cache_sizes_mb": {
                    "exact": round(exact_size, 2),
                    "normalized": round(normalized_size, 2),
                    "total": round(exact_size + normalized_size, 2)
                },
                "hit_rates": {
                    "exact": round((self.metrics["exact_hits"] / total_requests * 100) if total_requests > 0 else 0, 1),
                    "normalized": round((self.metrics["normalized_hits"] / total_requests * 100) if total_requests > 0 else 0, 1),
                    "semantic": round((self.metrics["semantic_hits"] / total_requests * 100) if total_requests > 0 else 0, 1),
                    "overall": round(overall_hit_rate, 1)
                },
                "performance": {
                    "total_requests": total_requests,
                    "avg_search_time_ms": round(self.metrics["avg_search_time_ms"], 2),
                    "semantic_searches": self.metrics["semantic_searches"],
                    "cache_misses": self.metrics["cache_misses"]
                },
                "config": {
                    "similarity_threshold": self.similarity_threshold,
                    "max_cache_size": self.max_cache_size
                }
            }
            
        except Exception as e:
            logger.error(f"Error getting cache statistics: {e}")
            return {}
    
    def clear_cache(self, cache_type: str = "all"):
        """Clear specific cache type or all caches"""
        import shutil
        
        try:
            if cache_type in ["all", "exact"] and self.exact_cache_dir.exists():
                shutil.rmtree(self.exact_cache_dir)
                self.exact_cache_dir.mkdir()
            
            if cache_type in ["all", "normalized"] and self.normalized_cache_dir.exists():
                shutil.rmtree(self.normalized_cache_dir)
                self.normalized_cache_dir.mkdir()
            
            if cache_type in ["all", "semantic"]:
                # Clear semantic data
                if self.semantic_embeddings_path.exists():
                    self.semantic_embeddings_path.unlink()
                if self.semantic_metadata_path.exists():
                    self.semantic_metadata_path.unlink()
                
                self.semantic_embeddings = np.empty((0, 1024), dtype=np.float32)
                self.semantic_metadata = {"texts": [], "hashes": [], "count": 0}
            
            if cache_type == "all":
                self.metrics = {
                    "exact_hits": 0,
                    "normalized_hits": 0,
                    "semantic_hits": 0,
                    "cache_misses": 0,
                    "total_requests": 0,
                    "avg_search_time_ms": 0.0,
                    "semantic_searches": 0,
                }
            
            logger.info(f"Cleared {cache_type} cache")
            
        except Exception as e:
            logger.error(f"Error clearing cache: {e}")
    
    def migrate_old_cache(self, old_cache_dir: Path) -> Dict[str, Any]:
        """
        Migrate embeddings from old cache structure to new optimized structure
        """
        migration_stats = {
            "total_files_found": 0,
            "migrated_count": 0,
            "failed_count": 0,
            "skipped_count": 0,
            "validation_errors": []
        }
        
        try:
            if not old_cache_dir.exists():
                logger.info(f"Legacy cache directory does not exist: {old_cache_dir}")
                return migration_stats
            
            logger.info(f"ðŸ”„ Starting migration from legacy cache: {old_cache_dir}")
            
            legacy_files = list(old_cache_dir.rglob("*.npy"))
            migration_stats["total_files_found"] = len(legacy_files)
            
            if not legacy_files:
                return migration_stats
            
            logger.info(f"Found {len(legacy_files)} legacy cache files to migrate")
            
            for npy_file in legacy_files:
                try:
                    embedding = np.load(npy_file)
                    
                    if not isinstance(embedding, np.ndarray) or embedding.size == 0:
                        raise ValueError("Invalid embedding array")
                    
                    if len(embedding.shape) != 1:
                        raise ValueError(f"Invalid embedding shape: {embedding.shape}")
                    
                    text_hash = npy_file.stem
                    
                    exact_path = self.exact_cache_dir / f"{text_hash}.npy"
                    normalized_path = self.normalized_cache_dir / f"{text_hash}.npy"
                    
                    if exact_path.exists():
                        try:
                            existing_embedding = np.load(exact_path)
                            if np.array_equal(embedding, existing_embedding):
                                migration_stats["skipped_count"] += 1
                                continue
                        except Exception:
                            pass
                    
                    np.save(exact_path, embedding)
                    np.save(normalized_path, embedding)
                    
                    migration_stats["migrated_count"] += 1
                    
                    if migration_stats["migrated_count"] % 100 == 0:
                        logger.info(f"Migration progress: {migration_stats['migrated_count']}/{len(legacy_files)}")
                        
                except Exception as e:
                    migration_stats["failed_count"] += 1
                    error_msg = f"Failed to migrate {npy_file.name}: {str(e)}"
                    migration_stats["validation_errors"].append(error_msg)
                    continue
            
            logger.info(f"âœ… Migration completed from {old_cache_dir}")
            logger.info(f"   ðŸ“Š Total files: {migration_stats['total_files_found']}")
            logger.info(f"   âœ… Migrated: {migration_stats['migrated_count']}")
            logger.info(f"   â­ï¸ Skipped: {migration_stats['skipped_count']}")
            logger.info(f"   âŒ Failed: {migration_stats['failed_count']}")
            
            return migration_stats
            
        except Exception as e:
            error_msg = f"Critical error during cache migration: {str(e)}"
            logger.error(error_msg)
            migration_stats["validation_errors"].append(error_msg)
            return migration_stats

# Global instance
_global_cache = None

def get_global_embedding_cache() -> OptimizedEmbeddingCache:
    """Get or create global embedding cache instance"""
    global _global_cache
    if _global_cache is None:
        cache_dir = Path("data/embeddings/global_cache_optimized")
        _global_cache = OptimizedEmbeddingCache(cache_dir, similarity_threshold=0.87)
    return _global_cache